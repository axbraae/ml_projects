{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, unique\n",
    "from numpy import ravel\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier # binary predictor if continuous use RandomForestRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful code snippets for Random Forest Modelling and others??\n",
    "\n",
    "# make a list of predictor names for later labelling\n",
    "ewr_X_names = list(ewr_predictors)\n",
    "\n",
    "# NB stratify on response as if it is skewed like so:\n",
    "# stratify on response as it is skewed 24% to 76%\n",
    "ewr_X_train, ewr_X_test, ewr_y_train, ewr_y_test = \\\n",
    "    train_test_split(ewr_predictors, ewr_response, test_size = 0.2, \n",
    "    random_state = 42,\n",
    "    stratify = ewr_response)\n",
    "\n",
    "# Use a dummy classifier/regressor for the baseline:\n",
    "clf_dummy = DummyClassifier(random_state=42)\n",
    "clf_dummy.fit(ewr_X_train, ewr_y_train)\n",
    "\n",
    "# model metrics for baseline model (rewrite as a function and move output to a log instead of print file)\n",
    "print(\"Accuracy\", (clf_dummy.score(ewr_X_test, ewr_y_test)) * 100)\n",
    "\n",
    "ewr_y_pred_test = clf_dummy.predict(ewr_X_test)\n",
    "ewr_y_pred_train = clf_dummy.predict(ewr_X_train)\n",
    "\n",
    "print(\"MAE train\", mean_absolute_error(ewr_y_train.astype('int'),\n",
    "                                        ewr_y_pred_train.astype('int')))\n",
    "print(\"MAE test\", mean_absolute_error(ewr_y_test.astype('int'),\n",
    "                                         ewr_y_pred_test.astype('int')))\n",
    "\n",
    "# confusion matrix for model - rewrite as function and move output to folder\n",
    "mat = confusion_matrix(ewr_y_test, ewr_y_pred_test)\n",
    "sns.heatmap(mat.T, square = True, annot = True, fmt = 'd', cbar = False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');\n",
    "\n",
    "# if interested in the features which impact on the model look at the below:\n",
    "\n",
    "# make feature importance plot\n",
    "features = ewr_X_names\n",
    "importances = ewr_model_final.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# customized number \n",
    "num_features = 15 \n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.title('Feature Importances')\n",
    "\n",
    "# only plot the customized number of features\n",
    "plt.barh(range(num_features), importances[indices[-num_features:]], color='#49006a', align='center')\n",
    "plt.yticks(range(num_features), [features[i] for i in indices[-num_features:]])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "\n",
    "# run a permutation_importance_plot to remove relationships which may be present between predictor features.\n",
    "\n",
    "result = permutation_importance(ewr_model_final, ewr_X_test, ewr_y_test, n_repeats=10,\n",
    "random_state=42, n_jobs=2)\n",
    "\n",
    "for i in result.importances_mean.argsort()[::-1]: \n",
    "    if result.importances_mean[i] - 2 * result.importances_std[i] > 0:\n",
    "        print(f\"{ewr_X_names[i]:<8}\"\n",
    "        f\"{result.importances_mean[i]:.3f}\"\n",
    "        f\" +/- {result.importances_std[i]:.3f}\")\n",
    "\n",
    "# make permutation importance plot\n",
    "features = ewr_X_names\n",
    "importances = result.importances_mean\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# customized number \n",
    "num_features = 15 \n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.title('Permutation Feature Importances')\n",
    "\n",
    "# only plot the customized number of features\n",
    "plt.barh(range(num_features), importances[indices[-num_features:]], color='#49006a', align='center')\n",
    "plt.yticks(range(num_features), [features[i] for i in indices[-num_features:]])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change print statements to logging statements in below then it is good!\n",
    "def model_metrics(input_pipe):\n",
    "    print(\"Training Accuracy\", (input_pipe.score(X_train, y_train)) * 100)\n",
    "    print(\"Test Accuracy\", (input_pipe.score(X_test, y_test)) * 100)\n",
    "\n",
    "    y_pred_test = input_pipe.predict(X_test)\n",
    "    y_pred_train = input_pipe.predict(X_train)\n",
    "    print(\"MAE train\", mean_absolute_error(y_train.astype('int'),\n",
    "                                        y_pred_train.astype('int')))\n",
    "    print(\"MAE test\", mean_absolute_error(y_test.astype('int'),\n",
    "                                         y_pred_test.astype('int')))\n",
    "    print(\"AUC train\", roc_auc_score(y_train, y_pred_train))\n",
    "    print(\"AUC test\", roc_auc_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for combined ROC plot\n",
    "\n",
    "plt.figure(0).clf()\n",
    "\n",
    "# add pipes for ROC\n",
    "\n",
    "pipes = [\n",
    "    {\n",
    "        'label':'Dummy Classifier', \n",
    "        'pipe': dummy_pipe, \n",
    "    }, \n",
    "    {\n",
    "        'label':'Random Forest', \n",
    "        'pipe': rf_pipe,\n",
    "    },\n",
    "    {\n",
    "        'label':'XGBoost', \n",
    "        'pipe': xgb_pipe,\n",
    "    },\n",
    "    {\n",
    "        'label':'XGBoost + parameters', \n",
    "        'pipe': xgb_param_pipe,\n",
    "    }\n",
    "]\n",
    "\n",
    "# iterate through pipelist\n",
    "\n",
    "for p in pipes:\n",
    "    pipe = p['pipe']\n",
    "    y_pred=pipe.predict(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pipe.predict_proba(X_test)[:,1])\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (p['label'], auc))\n",
    "    \n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipping_data = read_csv(\"\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "404edbd6b833d7d8b46a37381d861f94ac7ade6ced601140e9ec3868019abbf9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml_env_3-9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
